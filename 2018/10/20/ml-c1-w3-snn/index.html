<!DOCTYPE HTML>
<html lang="zh-CN">
<head>
    

<head>
    <meta charset="utf-8">
    <meta name="keywords" content="ml-c1-w3-snn, FunU">
    <meta name="description" content="1- Shallow Neural Network这一周的课程，主要是以Shallow Neural Network（即仅有一层hidden layer）为例介绍Neural Network。
1.1- Neural Networks Ov">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <title>ml-c1-w3-snn | FunU</title>
    <link rel="icon" type="image/png" href="/favicon.ico">

    <link rel="stylesheet" type="text/css" href="/libs/awesome/css/font-awesome.min.css">
    <link rel="stylesheet" type="text/css" href="/libs/materialize/css/materialize.min.css">
    <link rel="stylesheet" type="text/css" href="/libs/aos/aos.css">
    <link rel="stylesheet" type="text/css" href="/libs/animate/animate.min.css">
    <link rel="stylesheet" type="text/css" href="/libs/lightGallery/css/lightgallery.min.css">
    <link rel="stylesheet" type="text/css" href="/css/matery.css">
    <link rel="stylesheet" type="text/css" href="/css/my.css">

    <script src="/libs/jquery/jquery-2.2.0.min.js"></script>
<link rel="stylesheet" href="/css/prism-tomorrow.css" type="text/css">
<link rel="stylesheet" href="/css/prism-line-numbers.css" type="text/css"></head>

</head>

<body>

<header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="container">
            <div class="nav-wrapper">
                <div class="brand-logo">
                    <a href="/" class="waves-effect waves-light">
                        
                        <img src="/res/logo.png" class="logo-img hide-on-small-only">
                        
                        <span class="logo-span">FunU</span>
                    </a>
                </div>
                <a href="#" data-activates="mobile-nav" class="button-collapse"><i class="fa fa-navicon"></i></a>
<ul class="right">
    
    <li class="hide-on-med-and-down">
        <a href="/" class="waves-effect waves-light">
            
            <i class="fa fa-home"></i>
            
            <span>Index</span>
        </a>
    </li>
    
    <li class="hide-on-med-and-down">
        <a href="/tags" class="waves-effect waves-light">
            
            <i class="fa fa-tags"></i>
            
            <span>Tags</span>
        </a>
    </li>
    
    <li class="hide-on-med-and-down">
        <a href="/categories" class="waves-effect waves-light">
            
            <i class="fa fa-bookmark"></i>
            
            <span>Categories</span>
        </a>
    </li>
    
    <li class="hide-on-med-and-down">
        <a href="/archives" class="waves-effect waves-light">
            
            <i class="fa fa-archive"></i>
            
            <span>Archives</span>
        </a>
    </li>
    
    <li class="hide-on-med-and-down">
        <a href="/about" class="waves-effect waves-light">
            
            <i class="fa fa-user-circle-o"></i>
            
            <span>About</span>
        </a>
    </li>
    
    <li class="hide-on-med-and-down">
        <a href="/friends" class="waves-effect waves-light">
            
            <i class="fa fa-address-book"></i>
            
            <span>Friends</span>
        </a>
    </li>
    
    <li>
        <a id="toggleSearch" class="waves-effect waves-light">
            <i id="searchIcon" class="mdi-action-search"></i>
        </a>
    </li>

</ul>

<div class="side-nav" id="mobile-nav">

    <div class="mobile-head bg-color">
        
        <img src="/res/logo.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">FunU</div>
        <div class="logo-desc">
            
            刊落锋颖，一味恬静
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li>
            <a href="/" class="waves-effect waves-light">
                
                <i class="fa fa-fw fa-home"></i>
                
                Index
            </a>
        </li>
        
        <li>
            <a href="/tags" class="waves-effect waves-light">
                
                <i class="fa fa-fw fa-tags"></i>
                
                Tags
            </a>
        </li>
        
        <li>
            <a href="/categories" class="waves-effect waves-light">
                
                <i class="fa fa-fw fa-bookmark"></i>
                
                Categories
            </a>
        </li>
        
        <li>
            <a href="/archives" class="waves-effect waves-light">
                
                <i class="fa fa-fw fa-archive"></i>
                
                Archives
            </a>
        </li>
        
        <li>
            <a href="/about" class="waves-effect waves-light">
                
                <i class="fa fa-fw fa-user-circle-o"></i>
                
                About
            </a>
        </li>
        
        <li>
            <a href="/friends" class="waves-effect waves-light">
                
                <i class="fa fa-fw fa-address-book"></i>
                
                Friends
            </a>
        </li>
        
        
    </ul>

    <div class="social-link"><a href="https://github.com/" class="tooltipped" target="_blank" data-tooltip="-> Gituhb"
    data-position="top" data-delay="50">
    <i class="fa fa-github fa-lg"></i>
</a>
<a href="mailto:xxx@example.com" class="tooltipped" target="_blank" data-tooltip="-> Email" data-position="top"
    data-delay="50">
    <i class="fa fa-envelope fa-lg"></i>
</a>
<a href="https://twitter.com" class="tooltipped" data-tooltip="-> Twitter" data-position="top" data-delay="50">
    <i class="fa fa-twitter fa-lg"></i>
</a>
<a href="https://facebook.com" class="tooltipped" data-tooltip="-> Facebook" data-position="top" data-delay="50">
    <i class="fa fa-facebook-official fa-lg"></i>
</a>
<a href="https://instagram.com" class="tooltipped" data-tooltip="-> Ins" data-position="top" data-delay="50">
    <i class="fa fa-instagram fa-lg"></i>
</a>
</div>
</div>

            </div>
        </div>

        
    </nav>
</header>





<div class="bg-cover post-cover" style="background-image: url('/res/img/18.jpg')">
    <div class="container">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <div class="description center-align post-title">
                        ml-c1-w3-snn
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>



<main class="post-container content">

    
    <link rel="stylesheet" href="/libs/tocbot/tocbot.css">
<style>
    #articleContent h1,
    #articleContent h2,
    #articleContent h3,
    #articleContent h4,
    #articleContent h5,
    #articleContent h6 {
        padding-top: 76px;
        margin-top: -76px;
    }

    #articleContent h1 {
        line-height: 3.5rem;
    }

    #articleContent h2 {
        line-height: 3.2rem;
    }

    #articleContent h3 {
        line-height: 2.8rem;
    }

    #articleContent h4 {
        line-height: 2.5rem;
    }

    #articleContent h5 {
        line-height: 2.2rem;
    }

    #articleContent h6 {
        line-height: 1.9rem;
    }

    #articleContent :focus {
        outline: none;
    }

    .toc-fixed {
        position: fixed;
        top: 64px;
    }

    .toc-widget {
        padding-left: 20px;
    }

    .toc-widget .toc-title {
        margin: 35px 0 15px 0;
        padding-left: 17px;
        font-size: 1.5rem;
        font-weight: bold;
        line-height: 1.5rem;
    }

    .toc-widget ol {
        padding: 0;
        list-style: none;
    }

    #toc-content ol {
        padding-left: 10px;
    }

    #toc-content ol li {
        padding-left: 10px;
    }

    #toc-content .toc-link:hover {
        color: #42b983;
        font-weight: 700;
        text-decoration: underline;
    }

    #toc-content .toc-link::before {
        background-color: transparent;
        max-height: 25px;
    }

    #toc-content .is-active-link {
        color: #42b983;
    }

    #toc-content .is-active-link::before {
        background-color: #42b983;
    }
</style>
<div class="row">
    <div class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            
            <div class="article-tag">
                
                <a href="/tags/algorithms/" target="_blank">
                    <span class="chip bg-color">algorithms</span>
                </a>
                
            </div>
            
            <div class="post-info">
                

                <span class="post-date">
                    <i class="fa fa-clock-o fa-fw"></i>2018-10-20
                </span>
            </div>
        </div>
        <hr>
        <div class="card-content article-card-content">
            <div id="articleContent">
                <h1 id="1-Shallow-Neural-Network"><a href="#1-Shallow-Neural-Network" class="headerlink" title="1- Shallow Neural Network"></a>1- Shallow Neural Network</h1><p>这一周的课程，主要是以Shallow Neural Network（即仅有一层hidden layer）为例介绍Neural Network。</p>
<h2 id="1-1-Neural-Networks-Overview"><a href="#1-1-Neural-Networks-Overview" class="headerlink" title="1.1- Neural Networks Overview"></a>1.1- Neural Networks Overview</h2><ol>
<li>从Logistic Regression过渡到Neural Network。某种意义上看，<strong>Logistic Regression可以看成一个只有一层的neural network，</strong> 即没有hidden layer。</li>
<li>每一层的计算，类似于Logistic Regression:先计算z，再计算a。然后本层的a再作为下一层的输入计算。</li>
<li>重要的记号：不同layer的变量，在neural network中用上标中括号表示，比如：\(W^{[i]}\)表示第i层的权重。</li>
</ol>
<h2 id="1-2-Neural-Network-Representation"><a href="#1-2-Neural-Network-Representation" class="headerlink" title="1.2- Neural Network Representation"></a>1.2- Neural Network Representation</h2><p><img src="https://blog-1253739411.cos.ap-shanghai.myqcloud.com/content/2018/06/Screen-Shot-2018-06-13-at-08.20.29.jpg" alt="Screen-Shot-2018-06-13-at-08.20.29"></p>
<ol>
<li>Neural Network的组成：一个input layer，多个hidden layer，一个output layer</li>
<li>training set作为输入层，即第0层，因此有 \(X = a^{[0]}\)</li>
<li>每一层输入，上标[i]表示layer的层数，下标j表示neuron的序号（每层有多个neuron）</li>
<li>一个惯例，input layer不计算在neuron Network的层数里，并且input layer的上标是0。因此一个例子中的是neural network是2层的。</li>
<li>注意每层的参数w和b的维度。<strong>w的行数是本层的neuron的个数，行数是上一层neuron的个数。b是一个列向量，行数与w相同</strong>。</li>
</ol>
<h1 id="2-Computing-a-Neural-Network’s-Output"><a href="#2-Computing-a-Neural-Network’s-Output" class="headerlink" title="2- Computing a Neural Network’s Output"></a>2- Computing a Neural Network’s Output</h1><p>每个neuron的计算分为两步：z计算出线性组合，a计算激活函数：<br><img src="https://blog-1253739411.cos.ap-shanghai.myqcloud.com/content/2018/06/Screen-Shot-2018-06-13-at-08.28.31.jpg" alt="Screen-Shot-2018-06-13-at-08.28.31"></p>
<p>每层的计算向量化（注意这里只是一个数据样本x的情况，后面会讲如何扩展为m个数据样本的情况）：<br><img src="https://blog-1253739411.cos.ap-shanghai.myqcloud.com/content/2018/06/Screen-Shot-2018-06-13-at-08.32.51.jpg" alt="Screen-Shot-2018-06-13-at-08.32.51"></p>
<p>整理后，向量化的表示：</p>
<p>Given input x（a single training set）:</p>
<p>$$ z^{[1]} = W^{[1]}a^{[0]} + b^{[1]} $$<br>$$ a^{[1]} = \sigma(z^{[1]}) $$<br>$$ z^{[2]} = W^{[2]}a^{[1]} + b^{[2]} $$<br>$$ a^{[2]} = \sigma(z^{[2]}) $$</p>
<p>两点说明：</p>
<ol>
<li>对中括号上标，本层的输出a，和参数w、b的上标是一样的，而输入的上标则是上一层的。有些文档里可能会把w、b的上标保持和上一层一样，这是不同的习惯吧。</li>
<li>上面计算时的\(w^{[1]}_1\)做了转置，最终向量化时W没有没有转置，这是因为前者是按照logistic regression的习惯定义的，即w是列向量。而后者是W的定义，每一行是该层对应neuron的参数，已经行向量了。</li>
</ol>
<h2 id="2-1-Vectorizing-across-multiple-examples"><a href="#2-1-Vectorizing-across-multiple-examples" class="headerlink" title="2.1- Vectorizing across multiple examples"></a>2.1- Vectorizing across multiple examples</h2><p>前一节，已经强调了是针对一个training example的向量化，这一节要进一步对所有training example做向量化。</p>
<ol>
<li><p>首先，是非向量化的实现，即对m个training example进行迭代如下：<br><img src="https://blog-1253739411.cos.ap-shanghai.myqcloud.com/content/2018/06/Screen-Shot-2018-06-13-at-08.52.47.jpg" alt="Screen-Shot-2018-06-13-at-08.52.47"><br>即在上一节的公式上，对x、z、a添加小括号上标。z和a和x的维度是一样的，即和training example对应的，而参数w和b没有training example这一维度，或者说w和b是对所有m个training example计算结果做平均。</p>
</li>
<li><p>向量化</p>
<ul>
<li>先弄清每个矩阵的维度X,Z以及A。三个变量的维度是一致的，在column方向都表示不同的training example；在row的方向表示不同的neuron：<br><img src="https://blog-1253739411.cos.ap-shanghai.myqcloud.com/content/2018/06/Screen-Shot-2018-06-13-at-09.00.15.jpg" alt="Screen-Shot-2018-06-13-at-09.00.15"></li>
<li>基于上面对X,Z以及A的定义，有：<br>$$ Z^{[1]} = W^{[1]}A^{[0]} + b^{[1]} $$<br>$$ A^{[1]} = \sigma(Z^{[1]}) $$<br>$$ Z^{[2]} = W^{[2]}A^{[1]} + b^{[2]} $$<br>$$ A^{[2]} = \sigma(Z^{[2]}) $$</li>
</ul>
</li>
</ol>
<h2 id="2-2-Explanation-for-Vectorized-Implementation"><a href="#2-2-Explanation-for-Vectorized-Implementation" class="headerlink" title="2-2. Explanation for Vectorized Implementation"></a>2-2. Explanation for Vectorized Implementation</h2><p>其实就一句话：一个矩阵A乘以B，且B是列向量，那么B扩展为矩阵是没有问题的，结果也从一个列向量变成矩阵。</p>
<blockquote>
<p>When you took the different training examples and <strong>stacked them up in different columns</strong>, then the corresponding result is that you end up with <strong>the z’s also stacked at the columns</strong>.</p>
</blockquote>
<p>对于b来说，其实和X的个数无关，只要做broadcasting即可。</p>
<p>截止到上面，就是 <strong>(2 layer) Neural Network的forward propagation的向量化实现</strong>。</p>
<h2 id="2-3-Activation-functions"><a href="#2-3-Activation-functions" class="headerlink" title="2.3- Activation functions"></a>2.3- Activation functions</h2><ol>
<li><p>activation function是指将线性组合计算的z，进一步映射的<strong>non-linear function</strong>。比如sigmoid function就是将线性组合计算的z从实数域映射到了开区间(0,1)。</p>
</li>
<li><p>常用的activation function有：</p>
<ul>
<li><p>sigmoid function<br>$$a = \sigma(z) = \frac{1}{1+e^{-z}}$$<br>映射到开区间(0,1)</p>
<p><img src="https://blog-1253739411.cos.ap-shanghai.myqcloud.com/content/2018/06/sigmoid.png" alt="sigmoid"></p>
</li>
<li><p>tanh function: a shift verison a sigmoid function</p>
<p>$$a = tanh(z) =  \frac{e^{z}-e^{-z}}{e^{z}+e^{-z}}$$<br>映射到开区间(-1,1)</p>
<p><img src="https://blog-1253739411.cos.ap-shanghai.myqcloud.com/content/2018/06/tanh.png" alt="tanh"></p>
<p>在hidden layer中用tanh代替sigmoid，基本上效果会更好，因为tanh会产生一个均值为0的结果，相当于自动做了中心化处理。<br><strong>Andrew对选用sigmoid还是tanh的建议：hidden layer尽量用tanh而不是sigmoid，output layer根据输出需要可用sigmoid；另外每一层也可以选用不同的activation function。</strong></p>
</li>
<li><p>ReLU function<br>$$a = max(0,a) $$</p>
<p><img src="https://blog-1253739411.cos.ap-shanghai.myqcloud.com/content/2018/06/relu.jpeg" alt="relu"></p>
<p>sigmoid和tanh共同的不足是：<strong>当z很大或很小时，导数都趋近于0，这会导致梯度下降的速度比较慢</strong>。因此，又引入了ReLU function(<strong>Re</strong>ctified <strong>L</strong>inear <strong>U</strong>nit)，这个名字听起来挺牛逼，但实际表达式很简单，就是将<strong>线性函数小于0的部分做了修正（Rectifed）</strong>，最简单的修正就是直接让其等于0。<br>而且Andrew极力推荐ReLU：</p>
<blockquote>
<p>if you’re not sure what to use for your hidden layer, I would just use the relu activation function that’s what you see most people using these days. </p>
</blockquote>
<p>ReLU的一个好处是（我也不确定为什么是好处，Andrew只是说 in practice this works just fine）：ReLU在小于0的时候，导数为0。<br>ReLU另一个版本叫做Leaky ReLU，即在小于0的部分，修正为一个斜率很小的直线（ReLU修正为了斜率为0的直线）：<br><img src="https://blog-1253739411.cos.ap-shanghai.myqcloud.com/content/2018/06/leaky-relu.png" alt="leaky-relu"><br>Leaky ReLUE通常比ReLU效果更好，但实践中去不常用（为毛好的东西不用？）。<br>相比sigmoid或tanh，<strong>ReLU或Leaky ReLU计算的更快</strong> ，这也很显然，线性函数的导数就是一个常数。</p>
</li>
</ul>
</li>
<li><p>Activation Function的建议</p>
<ul>
<li>sigmoid：除非在output layer要做binary classification会用到，hidden layer几乎不会用。要用也是有限用tanh</li>
<li>默认或最常用的是ReLU（或者也可以尝试Leaky ReLU）</li>
<li>面对具体情况，也可以尝试不同的算法，再选择最好的。</li>
</ul>
</li>
</ol>
<h2 id="2-4-Why-do-you-need-non-linear-activation-functions"><a href="#2-4-Why-do-you-need-non-linear-activation-functions" class="headerlink" title="2.4- Why do you need non-linear activation functions?"></a>2.4- Why do you need non-linear activation functions?</h2><p>一句话解释：</p>
<blockquote>
<p>the composition of two linear functions is itself a linear function.</p>
</blockquote>
<p>因此无论你做多少层的network，其实与做一层没什么区别。</p>
<p>当然，在output layer是可以不用activation function，或者用linear activation function；这种情况一般是要求输出实数集结果（比如预测房价）。即便如此，在hidden layer还是要用non-linear activation function。</p>
<h2 id="2-5-Derivatives-of-activation-functions"><a href="#2-5-Derivatives-of-activation-functions" class="headerlink" title="2.5- Derivatives of activation functions"></a>2.5- Derivatives of activation functions</h2><p>三种activation function的导数：</p>
<ol>
<li>sigmoid function<br>$$\frac{d}{dz}\sigma(z) = \frac{d}{dz} \frac{1}{1+e^{-z}} = \sigma(z)(1 - \sigma(z))$$<br>推导参考：<br><a href="https://beckernick.github.io/sigmoid-derivative-neural-network/" target="_blank" rel="noopener">https://beckernick.github.io/sigmoid-derivative-neural-network/</a></li>
<li>tanh function<br>$$\frac{d}{dz}tanh(z)=1- tanh(z)^2$$<br>推导参考：<br><a href="http://math2.org/math/derivatives/more/hyperbolics.htm" target="_blank" rel="noopener">http://math2.org/math/derivatives/more/hyperbolics.htm</a><br><a href="https://math.stackexchange.com/questions/741050/hyperbolic-functions-derivative-of-tanh-x" target="_blank" rel="noopener">https://math.stackexchange.com/questions/741050/hyperbolic-functions-derivative-of-tanh-x</a></li>
<li>ReLU function<br>这个就很简单了，唯一要注意0的时候不可导，因此做个修正，直接定义0点的导数为0。</li>
</ol>
<h2 id="2-6-Gradient-descent-for-Neural-Networks"><a href="#2-6-Gradient-descent-for-Neural-Networks" class="headerlink" title="2.6- Gradient descent for Neural Networks"></a>2.6- Gradient descent for Neural Networks</h2><ol>
<li>gradient descent的关键是求cost function对参数的偏导数</li>
<li>求导过程使用的是Backpropagation<ol>
<li>首先做forward propagation，求解出每一层的输出A</li>
<li>然后向后，逐层求解对每一层参数的偏导数</li>
</ol>
</li>
<li>课程里并没有backpropagation的推导，只是给出了结果：<br><img src="https://blog-1253739411.cos.ap-shanghai.myqcloud.com/content/2018/06/grad_summary.png" alt="grad_summary"><br>Backpropagation具体推导过程，非常推荐这篇文章：<a href="http://http://neuralnetworksanddeeplearning.com/chap2.html" target="_blank" rel="noopener">How the backpropagation algorithm works</a></li>
</ol>
<h2 id="2-7-Backpropagation-intuition-optional"><a href="#2-7-Backpropagation-intuition-optional" class="headerlink" title="2.7- Backpropagation intuition (optional)"></a>2.7- Backpropagation intuition (optional)</h2><ol>
<li>首先用logistic regression解释了backpropagation</li>
</ol>
<p><img src="https://blog-1253739411.cos.ap-shanghai.myqcloud.com/content/2018/06/Screen-Shot-2018-06-14-at-20.47.47.jpg" alt="Screen-Shot-2018-06-14-at-20.47.47"></p>
<ol start="2">
<li>然后在2 layer Neuro Networks上解释<br><img src="https://blog-1253739411.cos.ap-shanghai.myqcloud.com/content/2018/06/Screen-Shot-2018-06-14-at-20.49.18.jpg" alt="Screen-Shot-2018-06-14-at-20.49.18"></li>
</ol>
<h2 id="2-8-Random-Initialization"><a href="#2-8-Random-Initialization" class="headerlink" title="2.8- Random Initialization"></a>2.8- Random Initialization</h2><p>与logistic regression不同，初始化参数不可固定为0，而是每个参数都要随机初始化。</p>
<p>主要原因是：<strong>如果每个参数w和b都是0，则同一层的每个neuron计算结果完全一样</strong>（输入一样a，参数一样w，则z一样）；接下来反向传播时的偏导数也一样，下一轮迭代同一层的每个neuron的w又是一样的。这样整个neural Network上每一层的neuron是同质的，自然不会有好的performance。</p>
<p>不过，对b参数，可以都初始化为0。</p>
<p>另外需要注意，虽然w是随机初始化，<strong>但最好使用较小的随机数</strong>。主要是避免让z的计算值过大，导致activation function对z的偏导数趋于0，导致Gradient descent下降较慢。<br>通常的做法是对random的值乘以一个比率，比如0.01（但具体怎么选这个比率，也要根据情况而定，这应该又是一个超参了）：<br>$$ W^{[1]} = np.random.randn((2,2))  * 0.01$$</p>
<h2 id="2-9-The-general-methodology-to-build-a-Neural-Network-is-to"><a href="#2-9-The-general-methodology-to-build-a-Neural-Network-is-to" class="headerlink" title="2.9- The general methodology to build a Neural Network is to:"></a>2.9- The general methodology to build a Neural Network is to:</h2><ol>
<li>Define the neural network structure ( # of input units,  # of hidden units, etc). </li>
<li>Initialize the model’s parameters</li>
<li>Loop:<ul>
<li>Implement forward propagation</li>
<li>Compute loss</li>
<li>Implement backward propagation to get the gradients</li>
<li>Update parameters (gradient descent)<h1 id="3-Heros-of-Deep-learning-Ian-Goodfellow-interview"><a href="#3-Heros-of-Deep-learning-Ian-Goodfellow-interview" class="headerlink" title="3- Heros of Deep learning: Ian Goodfellow interview"></a>3- Heros of Deep learning: Ian Goodfellow interview</h1></li>
</ul>
</li>
</ol>
<p><img src="https://blog-1253739411.cos.ap-shanghai.myqcloud.com/content/2018/06/Screen-Shot-2018-06-12-at-18.20.12.jpg" alt="Screen-Shot-2018-06-12-at-18.20.12"></p>
<ol>
<li>这位就是大名鼎鼎的花书<a href="http://www.deeplearningbook.org/" target="_blank" rel="noopener"><em>Deep Learning</em></a>的作者，原以为是个白胡子老头，没想到竟然是个85后。</li>
<li>学习了Andrew Ng 的 Machine Learning 开始对AI感兴趣。<blockquote>
<p>I started to have a very strong intuition that deep learning was the way to go in the future.</p>
</blockquote>
</li>
<li>How did you come up with GANs (<strong>G</strong>enerative <strong>A</strong>dversarial <strong>N</strong>etwork<strong>s</strong>)?<ul>
<li>GANs 可以自动生成更多的训练数据。</li>
<li>GANs were introduced in a paper by Ian Goodfellow and other researchers at the University of Montreal, including Yoshua Bengio, in 2014. </li>
<li><blockquote>
<p>I implemented it <strong>around midnight</strong> after going home from the bar where my friend had his going-away party.</p>
</blockquote>
</li>
</ul>
</li>
<li>有一次人都差点挂了，第一想到的还是怎么把自己的研究尽快弄出来。</li>
<li>和自己的两个 Ph.D. 导师共同编写了<a href="http://www.deeplearningbook.org/" target="_blank" rel="noopener"><em>Deep Learning</em></a>这本书。<ul>
<li>特别提到了中文版和英文版卖的都很好。</li>
<li>与其他书不同，本书中特别介绍了 Deep Learning 需要的数学知识。</li>
<li><blockquote>
<p>If you want to be a really excellent practitioner, you’ve got to <strong>master the basic math that underlies the whole approach in the first place</strong>.</p>
</blockquote>
</li>
</ul>
</li>
<li><blockquote>
<p>And now, we’re at a point where there are so many different paths open that someone who wants to get involved in AI, maybe the hardest problem they face is <strong>choosing which path they want to go down</strong>. </p>
</blockquote>
</li>
<li>Advice for someone wanting to get into AI.<ul>
<li>Ph.D. is not a absolute requirement any more.</li>
<li>One way that you could get a lot of attention is to <strong>write good code and put it on GitHub</strong>.</li>
<li>I think if you learned by reading the book, it’s really important to also <strong>work on a project at the same time</strong>.</li>
</ul>
</li>
<li>Work on adversarial examples. <ul>
<li>It’s a new field called <strong>machine learning security</strong>.</li>
</ul>
</li>
</ol>
<p><em>（另外，这家伙居然几乎不眨眼，也不转动眼球。）</em></p>
<p>系列笔记：<a href="/tag/deeplearning-ai-notes/">deeplearning.ai深度学习笔记系列首页</a></p>

            </div>
            <hr/>

            
            <style>
    #reward {
        margin: 40px 0;
        text-align: center;
    }

    #reward .reward-link {
        font-size: 1.88rem;
    }

    #reward .btn-floating:hover {
        box-shadow: 0 6px 12px rgba(0, 0, 0, 0.2), 0 5px 15px rgba(0, 0, 0, 0.2);
    }

    #rewardModal {
        width: 320px;
        height: 350px;
    }

    #rewardModal .reward-title {
        margin: 15px auto;
        padding-bottom: 5px;
    }

    #rewardModal .modal-content {
        padding: 10px;
    }

    #rewardModal .close {
        position: absolute;
        right: 15px;
        top: 15px;
        color: rgba(0, 0, 0, 0.5);
        font-size: 1.3rem;
        line-height: 20px;
        cursor: pointer;
    }

    #rewardModal .reward-tabs {
        margin: 0 auto;
        width: 210px;
    }

    .reward-tabs .tabs {
        height: 38px;
        margin: 10px auto;
        padding-left: 0;
    }

    .reward-tabs .tabs .tab {
        height: 38px;
        line-height: 38px;
    }

    .reward-tabs .tab a {
        color: #fff;
        background-color: #ccc;
    }

    .reward-tabs .tab a:hover {
        color: #fff;
    }

    .reward-tabs .wechat-tab .active {
        color: #fff;
        background-color: #22AB38;
    }

    .reward-tabs .alipay-tab .active {
        color: #fff;
        background-color: #019FE8;
    }

    .reward-tabs .reward-img {
        width: 210px;
        height: 210px;
    }
</style>

<div id="reward">
    <a class="reward-link btn-floating btn-large waves-effect waves-light red">赏</a>

    <!-- Modal Structure -->
    <div id="rewardModal" class="modal">
        <div class="modal-content">
            <a class="close"><i class="fa fa-close"></i></a>
            <h4 class="reward-title">感谢您的支持！</h4>
            <div class="reward-content">
                <div class="reward-tabs">
                    <ul class="tabs">
                        <li class="tab wechat-tab waves-effect waves-light"><a class="active" href="#wechat">微信</a></li>
                        <li class="tab alipay-tab waves-effect waves-light"><a href="#alipay">支付宝</a></li>
                    </ul>
                    <div id="wechat">
                        <img src="/res/reward/wechat.png" class="reward-img" alt="微信打赏二维码">
                    </div>
                    <div id="alipay">
                        <img src="/res/reward/alipay.jpg" class="reward-img" alt="支付宝打赏二维码">
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>

<script>
    $(function () {
        $('#reward .reward-link').on('click', function () {
            $('#rewardModal').openModal();
        });

        $('#rewardModal .close').on('click', function () {
            $('#rewardModal').closeModal();
        });
    });
</script>
            

            <link rel="stylesheet" type="text/css" href="/libs/share/css/share.min.css">

<div id="article-share">
    
    <div class="social-share" data-disabled="qzone" data-wechat-qrcode-helper="<p>微信里点“发现”->“扫一扫”二维码便可查看分享。</p>"></div>
    
</div>

<script src="/libs/share/js/social-share.min.js"></script>

            <div class="reprint">
                <p>
                    <span class="reprint-tip">转载请注明: </span>
                    <a href="https://dybl.github.io" class="b-link-green">FunU</a>
                    <i class="fa fa-angle-right fa-lg fa-fw text-color"></i>
                    <a href="/2018/10/20/ml-c1-w3-snn/" class="b-link-green">ml-c1-w3-snn</a>
                </p>
            </div>
        </div>
    </div>

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">上一篇</div>
            <div class="card">
                <a href="/2018/10/21/ml-c2-w1-set/">
                    <div class="card-image">
                        
                        
                        <img src="/res/img/8.jpg" class="responsive-img" alt="ml-c2-w1-set">
                        
                        <span class="card-title">ml-c2-w1-set</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary">这是Course 2的内容，涉及：

训练集、开发集、测试集的概念
Bias/Variance问题
如何通过泛化（regularization）算法，解决High variance问题，以及常用的集中regularization算法。
最小</div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="fa fa-clock-o fa-fw icon-date"></i>2018-10-21
                        </span>
                        <span class="publish-author">
                            
                            <i class="fa fa-user fa-fw"></i>
                            Junjc9
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/tags/algorithms/" target="_blank">
                        <span class="chip bg-color">algorithms</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">下一篇</div>
            <div class="card">
                <a href="/2018/10/19/ml-c1-w2-basic/">
                    <div class="card-image">
                        
                        
                        <img src="/res/img/0.jpg" class="responsive-img" alt="ml-c1-w2-basic">
                        
                        <span class="card-title">ml-c1-w2-basic</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary">
Basics of Neural Network Programming

1- Logistic Regression as a Neural Network1.1- Binary Classification
Example: 给一张</div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="fa fa-clock-o fa-fw icon-date"></i>2018-10-19
                            </span>
                        <span class="publish-author">
                            
                            <i class="fa fa-user fa-fw"></i>
                            Junjc9
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/tags/algorithms/" target="_blank">
                        <span class="chip bg-color">algorithms</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>
</div>
    </div>
    <div class="col l3 hide-on-med-and-down">
        <div class="toc-widget">
            <div class="toc-title">目录</div>
            <div id="toc-content">

            </div>
        </div>
    </div>
</div>

<script src="/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingSelector: 'h2, h3, h4'
        });

        // modify the toc link href to support Chinese.
        let i = 0;
        let tocHeading = 'toc-heading-';
        $('#toc-content a').each(function () {
            $(this).attr('href', '#' + tocHeading + (++i));
        });

        // modify the heading title id to support Chinese.
        i = 0;
        $('#articleContent').children('h2, h3, h4').each(function () {
            $(this).attr('id', tocHeading + (++i));
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });
    });
</script>
    

</main>


<footer class="page-footer bg-color">
    <div class="container row center-align">
        <div class="col s12 m8 l8 copy-right">
            2018 &copy;<a href="http://dybl.github.io" target="_blank"> FunU co.,ltd</a>
        </div>
        <div class="col s12 m4 l4 social-link"><a href="https://github.com/" class="tooltipped" target="_blank" data-tooltip="-> Gituhb"
    data-position="top" data-delay="50">
    <i class="fa fa-github fa-lg"></i>
</a>
<a href="mailto:xxx@example.com" class="tooltipped" target="_blank" data-tooltip="-> Email" data-position="top"
    data-delay="50">
    <i class="fa fa-envelope fa-lg"></i>
</a>
<a href="https://twitter.com" class="tooltipped" data-tooltip="-> Twitter" data-position="top" data-delay="50">
    <i class="fa fa-twitter fa-lg"></i>
</a>
<a href="https://facebook.com" class="tooltipped" data-tooltip="-> Facebook" data-position="top" data-delay="50">
    <i class="fa fa-facebook-official fa-lg"></i>
</a>
<a href="https://instagram.com" class="tooltipped" data-tooltip="-> Ins" data-position="top" data-delay="50">
    <i class="fa fa-instagram fa-lg"></i>
</a>
</div>
    </div>
</footer>

<div class="progress-bar"></div>


<!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title">搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input" autofocus="">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script src="/js/search.js"></script>
<script type="text/javascript">
    searchFunc("/" + "search.xml", 'searchInput', 'searchResult');
</script>
<!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fa fa-angle-up"></i>
    </a>
</div>


<script src="/libs/materialize/js/materialize.min.js"></script>
<script src="/libs/masonry/masonry.pkgd.min.js"></script>
<script src="/libs/aos/aos.js"></script>
<script src="/libs/scrollprogress/scrollProgress.min.js"></script>
<script src="/libs/lightGallery/js/lightgallery-all.min.js"></script>
<script src="/js/matery.js"></script>
<!-- Global site tag (gtag.js) - Google Analytics -->


</body>
</html>