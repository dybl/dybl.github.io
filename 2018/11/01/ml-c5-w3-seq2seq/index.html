<!DOCTYPE HTML>
<html lang="">
<head>
    

<head>
    <meta charset="utf-8">
    <meta name="keywords" content="ml-c5-w3-seq2seq, FunU">
    <meta name="description" content="
本周是deeplearning.ai系列的最后一节课，以机器翻译为例，介绍了seq2seq模型，扩展分析了beam search、bleu score和attention model。最后简要介绍了语音识别和触发词检测。

1- Vari">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <title>ml-c5-w3-seq2seq | FunU</title>
    <link rel="icon" type="image/png" href="/favicon.png">

    <link rel="stylesheet" type="text/css" href="/libs/awesome/css/font-awesome.min.css">
    <link rel="stylesheet" type="text/css" href="/libs/materialize/css/materialize.min.css">
    <link rel="stylesheet" type="text/css" href="/libs/aos/aos.css">
    <link rel="stylesheet" type="text/css" href="/libs/animate/animate.min.css">
    <link rel="stylesheet" type="text/css" href="/libs/lightGallery/css/lightgallery.min.css">
    <link rel="stylesheet" type="text/css" href="/css/matery.css">
    <link rel="stylesheet" type="text/css" href="/css/my.css">

    <script src="/libs/jquery/jquery-2.2.0.min.js"></script>
<link rel="stylesheet" href="/css/prism-atom-dark.css" type="text/css">
<link rel="stylesheet" href="/css/prism-line-numbers.css" type="text/css"></head>

</head>

<body>

<header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="container">
            <div class="nav-wrapper">
                <div class="brand-logo">
                    <a href="/" class="waves-effect waves-light">
                        
                        <img src="/medias/logo.png" class="logo-img hide-on-small-only">
                        
                        <span class="logo-span">FunU</span>
                    </a>
                </div>
                <a href="#" data-activates="mobile-nav" class="button-collapse"><i class="fa fa-navicon"></i></a>
<ul class="right">
    
    <li class="hide-on-med-and-down">
        <a href="/" class="waves-effect waves-light">
            
            <i class="fa fa-anchor"></i>
            
            <span>Index</span>
        </a>
    </li>
    
    <li class="hide-on-med-and-down">
        <a href="/tags" class="waves-effect waves-light">
            
            <i class="fa fa-tags"></i>
            
            <span>Tags</span>
        </a>
    </li>
    
    <li class="hide-on-med-and-down">
        <a href="/categories" class="waves-effect waves-light">
            
            <i class="fa fa-snowflake-o"></i>
            
            <span>Categories</span>
        </a>
    </li>
    
    <li class="hide-on-med-and-down">
        <a href="/archives" class="waves-effect waves-light">
            
            <i class="fa fa-dot-circle-o"></i>
            
            <span>Archives</span>
        </a>
    </li>
    
    <li class="hide-on-med-and-down">
        <a href="/about" class="waves-effect waves-light">
            
            <i class="fa fa-user"></i>
            
            <span>About</span>
        </a>
    </li>
    
    <li>
        <a id="toggleSearch" class="waves-effect waves-light">
            <i id="searchIcon" class="mdi-action-search"></i>
        </a>
    </li>

</ul>

<div class="side-nav" id="mobile-nav">

    <div class="mobile-head bg-color">
        
        <img src="/medias/logo.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">FunU</div>
        <div class="logo-desc">
            
            Do not show me the f***ing source code!
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li>
            <a href="/" class="waves-effect waves-light">
                
                <i class="fa fa-fw fa-anchor"></i>
                
                Index
            </a>
        </li>
        
        <li>
            <a href="/tags" class="waves-effect waves-light">
                
                <i class="fa fa-fw fa-tags"></i>
                
                Tags
            </a>
        </li>
        
        <li>
            <a href="/categories" class="waves-effect waves-light">
                
                <i class="fa fa-fw fa-snowflake-o"></i>
                
                Categories
            </a>
        </li>
        
        <li>
            <a href="/archives" class="waves-effect waves-light">
                
                <i class="fa fa-fw fa-dot-circle-o"></i>
                
                Archives
            </a>
        </li>
        
        <li>
            <a href="/about" class="waves-effect waves-light">
                
                <i class="fa fa-fw fa-user"></i>
                
                About
            </a>
        </li>
        
        
    </ul>

    <div class="social-link"><a href="mailto:xxx@example.com" class="tooltipped" target="_blank" data-tooltip="-> Email" data-position="top"
    data-delay="50">
    <i class="fa fa-envelope fa-lg"></i>
</a>
<a href="https://twitter.com" class="tooltipped" data-tooltip="-> Twitter" data-position="top" data-delay="50">
    <i class="fa fa-twitter fa-lg"></i>
</a>
<a href="https://facebook.com" class="tooltipped" data-tooltip="-> Facebook" data-position="top" data-delay="50">
    <i class="fa fa-facebook-official fa-lg"></i>
</a>
<a href="https://instagram.com" class="tooltipped" data-tooltip="-> Ins" data-position="top" data-delay="50">
    <i class="fa fa-instagram fa-lg"></i>
</a>
</div>
</div>



            </div>
        </div>

        
    </nav>
</header>



<div class="bg-cover post-cover" style="background-image: url('/medias/featureimages/0.jpg')">
    <div class="container">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <div class="description center-align post-title">
                        ml-c5-w3-seq2seq
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>



<main class="container content">

    
    <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            
            <div class="article-tag">
                
                <a href="/tags/algorithms/" target="_blank">
                    <span class="chip bg-color">algorithms</span>
                </a>
                
            </div>
            
            <div class="post-info">
                

                <span class="post-date">
                    <i class="fa fa-clock-o fa-fw"></i>2018-11-01
                </span>
            </div>
        </div>
        <hr>
        <div class="card-content article-card-content">
            <div id="articleContent">
                <blockquote>
<pre><code>本周是deeplearning.ai系列的最后一节课，以机器翻译为例，介绍了seq2seq模型，扩展分析了beam search、bleu score和attention model。最后简要介绍了语音识别和触发词检测。
</code></pre></blockquote>
<h1 id="1-Various-sequence-to-sequence-architectures"><a href="#1-Various-sequence-to-sequence-architectures" class="headerlink" title="1- Various sequence to sequence architectures"></a>1- Various sequence to sequence architectures</h1><h2 id="1-1-Basic-Models"><a href="#1-1-Basic-Models" class="headerlink" title="1.1- Basic Models"></a>1.1- Basic Models</h2><p>这周将学习<strong>序列到序列（seq2seq, sequence to sequence）模型</strong>，seq2seq在序列到序列的转换方面非常有用，尤其是机器翻译和语音识别。</p>
<p>seq2seq模型主要来源于如下两篇论文：</p>
<ul>
<li><a href="https://arxiv.org/pdf/1409.3215.pdf" target="_blank" rel="noopener">Sutskever et al., 2014. Sequence to sequence learning with neural networks</a></li>
<li><a href="https://arxiv.org/pdf/1406.1078.pdf" target="_blank" rel="noopener">Cho et al., 2014. Learning phrase representations using RNN encoder-decoder for statistical machine translation</a></li>
</ul>
<p>我们先从最基本的模型开始。假设我们有一个机器翻译的任务，将法语翻译成英语，比如：</p>
<blockquote>
<p>法语：Jane  visite  l’Afrique  en  septembre.<br>英语：Jane  is  visiting  Africa  in  September.</p>
</blockquote>
<p>通常，我们用\(x\)和\(y\)将输入和输出表示为序列，如下：</p>
<p><img src="https://blog-1253739411.cos.ap-shanghai.myqcloud.com/content/2018/07/Xnip2018-07-30_20-03-40.jpg" alt="Xnip2018-07-30_20-03-40"></p>
<p><strong>我们的目标就是，训练一个神经网络，输入序列\(x\)，输出序列\(y\)。</strong> 首先，我们先创建一个称为<strong>编码器（encoder）</strong> 的RNN（具体可以是GRU或LSTM），并把法语序列输入到该网络。当encoder将所有单词都“吞下”后，在最后的时间步的状态输入到一个称为<strong>解码器（decoder）</strong> 的RNN，该RNN输出翻译后的英语，如下图：</p>
<p><img src="https://blog-1253739411.cos.ap-shanghai.myqcloud.com/content/2018/07/Xnip2018-07-30_21-04-14.jpg" alt="Xnip2018-07-30_21-04-14"></p>
<p>在给出足够的法语和英语对应文本的情况下，这个模型的效果相当不错。该模型简单的使用一个编码网络，将输入的法语编码，然后用一个解码网络生成对应的英语翻译。</p>
<p>和上面非常类似的架构，同时也应用于图像描述（image captioning）。比如，下面输入一张图片，给出图片的描述（即看图说话）：</p>
<p><img src="https://blog-1253739411.cos.ap-shanghai.myqcloud.com/content/2018/07/Xnip2018-07-30_20-31-42.jpg" alt="Xnip2018-07-30_20-31-42"></p>
<p>结合之前我们学习的CNN网络，我们先用CNN对图片进行编码，比如用预训练好的AlexNet得到4096维的编码，然后将编码输入到RNN作为解码器，输出对图像的描述：</p>
<p><img src="https://blog-1253739411.cos.ap-shanghai.myqcloud.com/content/2018/07/Xnip2018-07-30_21-03-02.jpg" alt="Xnip2018-07-30_21-03-02"></p>
<p>上面的图片描述模型，几乎被多篇论文同时提出：</p>
<ul>
<li><a href="https://arxiv.org/pdf/1412.6632.pdf" target="_blank" rel="noopener">Mao et. al., 2014. Deep captioning with multimodal recurrent neural networks</a></li>
<li><a href="https://arxiv.org/pdf/1411.4555.pdf" target="_blank" rel="noopener">Vinyals et. al., 2014. Show and tell: Neural image caption generator</a></li>
<li><a href="https://arxiv.org/pdf/1412.2306.pdf" target="_blank" rel="noopener">Karpathy and Li, 2015. Deep visual-semantic alignments for generating image descriptions</a></li>
</ul>
<p>上面的基本seq2seq模型，和之前讨论的使用语言模型产生新文本还有差别。<strong>主要在于，我们并不下午随机的选择翻译或图片描述，而是希望选择可能性最大的</strong>。接下来我们将介绍如何做到。</p>
<h2 id="1-2-Picking-the-most-likely-sentence"><a href="#1-2-Picking-the-most-likely-sentence" class="headerlink" title="1.2- Picking the most likely sentence"></a>1.2- Picking the most likely sentence</h2><p>seq2seq翻译模型和先前讨论的语言模型既有些相似，也有很大不同。<strong>我们可以将机器翻译看做构建一个条件语言模型（conditional language model）</strong>。</p>
<p>下图是两个模型的对比，我们可以看出机器翻译模型的decoder部分和语言模型十分类似，唯一差别在于，机器翻译的最初输入状态来源于encoder，而语言模型的输入固定为\(a^{\langle  0 \rangle}\)。</p>
<p><img src="https://blog-1253739411.cos.ap-shanghai.myqcloud.com/content/2018/07/Xnip2018-07-30_21-08-46.jpg" alt="Xnip2018-07-30_21-08-46"></p>
<p>语言模型计算的是概率：</p>
<p>$$P(y^{\langle  1 \rangle},…,y^{\langle T_y\rangle})$$</p>
<p>而机器翻译模型，计算的是条件概率（这也是其称为条件语言模型的原因）：</p>
<p>$$P(y^{\langle  1 \rangle},…,y^{\langle T_y\rangle}| x^{\langle  1 \rangle},…,x^{\langle T_x\rangle})$$</p>
<p>解码器随机采样，可能导致翻译结果时好时坏，因此我们不能随机的抽样出翻译结果，<strong>而是找到令上面条件概率最大化的翻译</strong>，即：</p>
<p>$$\text{arg} \ \text{max}_{y^{\langle  1 \rangle}, …, y^{\langle T_y\rangle}}P(y^{\langle  1 \rangle}, …, y^{\langle T_y\rangle} | x)$$</p>
<p>因此在开发机器学习系统时，你要做的事情之一就是使用算法<strong>求出使得上面条件概率最大的\(y\)。其中最常用的算法之一是集束搜索（Beam Search）</strong>。</p>
<p>在学习Beam Search之前，我们先看一下为什么不用贪心搜索（greedy search），贪心搜索只是最大化解码器当期时间步的条件概率，而我们希望的是整个语句的条件概率最大。贪心算法很容易陷入次优解，所以不适合。</p>
<p>与greedy search对应的是另外一个极端：对全体空间搜索。比如翻译的语句长度是10，词汇表是10000，则需要搜索\(10000^{10}\)个句子，这个搜索量和让猴子敲出莎士比亚文集没什么区别，显然是不现实的。</p>
<h2 id="1-3-Beam-Search"><a href="#1-3-Beam-Search" class="headerlink" title="1.3- Beam Search"></a>1.3- Beam Search</h2><p>我们还以上面的法语翻译为例，讲解Beam Search的步骤：</p>
<ul>
<li>首先看第一个单词的条件概率\(P(\hat y^{\langle  1 \rangle}|x)\)，并<strong>选择概率最大的3个单词作为候选</strong>，这里个数3称为<strong>集束宽（beam width）</strong>，代表解码器中每个时间步候选的单词个数，记作\(B\)。具体上面的例子，可能选到了3个候选单词是：in, jane, september。<br>  <img src="https://blog-1253739411.cos.ap-shanghai.myqcloud.com/content/2018/07/Xnip2018-07-31_09-29-29.jpg" alt="Xnip2018-07-31_09-29-29"></li>
<li>然后分别考虑第1个单词是上面3个候选单词之一的条件下，第2个单词的概率\(P(\hat y^{\langle  2 \rangle}|x, \hat y^{\langle  1 \rangle})\)。比如第一个单词是in的概率\(P(\hat y^{\langle  2 \rangle}|x, \text{‘in’})\)。然后再选出构成前两个单词概率最大的3个组合，其概率计算为：<br>  $$P(\hat y^{\langle  1 \rangle}, \hat y^{\langle  2 \rangle}|x) = P(\hat y^{\langle  1 \rangle}|x) P(\hat y^{\langle  2 \rangle}|x, \hat y^{\langle  1 \rangle})$$<br>  由于\(B=3\)，因此我们在这一步会考虑3x10000种组合（第一步对应3个单词，第二步对应词汇表10000个单词），并选择其中概率最大的3个。最终可能的候选是：in september, jane is, jane visits（第一个单词是september的情况已经被剔除了）。我们将30000的搜索范围，收缩为3个候选。<br>  <img src="https://blog-1253739411.cos.ap-shanghai.myqcloud.com/content/2018/07/Xnip2018-07-31_09-30-46.jpg" alt="Xnip2018-07-31_09-30-46"></li>
<li>第3步和第2步类似，可能继续得到3个候选：in september jane, jane is visiting, jane visits africa<br>  <img src="https://blog-1253739411.cos.ap-shanghai.myqcloud.com/content/2018/07/Xnip2018-07-31_09-26-34.jpg" alt="Xnip2018-07-31_09-26-34"></li>
</ul>
<p>特别的，<strong>如果\(B=1\)，beam search就退化为了贪心搜索</strong>。</p>
<h2 id="1-4-Refinements-to-Beam-Search"><a href="#1-4-Refinements-to-Beam-Search" class="headerlink" title="1.4- Refinements to Beam Search"></a>1.4- Refinements to Beam Search</h2><p>下面介绍一些对Beam search算法的优化。</p>
<ol>
<li>长度归一化（length normalization）</li>
</ol>
<p>我们知道Beam Search的优化目标是：<br>$$\text{arg} \ \text{max}_{y^{\langle  1 \rangle}, …, y^{\langle T_y\rangle}}P(y^{\langle  1 \rangle}, …, y^{\langle T_y\rangle} | x)$$</p>
<p>展开为条件概率的乘积：<br>$$\text{arg max} \prod^{T_y}_{t=1} P( y^{\langle  t \rangle} | x, y^{\langle  1 \rangle}, …, y^{\langle  t-1 \rangle})$$</p>
<p>实践中，每个因子通常是远小于1的概率，经过一连串的乘积会导致结果为一个很小很小的数字，导致<strong>下溢（underflow）</strong>，即计算机没法精确表示。因此实际操作中，为<strong>将优化目标取对数</strong>，规避下溢问题。我知道对数函数是单调递增的，因此两个优化目标等价，即：</p>
<p>$$\text{arg max} \sum^{T_y}_{t=1} \log P( y^{\langle  t \rangle} | x,  y^{\langle  1 \rangle}, …,  y^{\langle  t-1 \rangle})$$</p>
<p>观察上面的优化目标，可以发现优化目标倾向于选择长度短的输出序列，因为每个条件概率都小于1，因此序列越短总的概率越大，这并不是我们希望的。因此我们再增加一个系数对太短的语句惩罚，简单点就用输出序列的总长度做一下平均：</p>
<p>$$\text{arg max} \frac{1}{T_y} \sum^{T_y}_{t=1} \log P( y^{\langle  t \rangle} | x,  y^{\langle  1 \rangle}, …,  y^{\langle  t-1 \rangle})$$</p>
<p>实践中，一般我们会再加一个更柔和的处理方法，即\(T_y\)上加上指数\(\alpha\)：</p>
<p>$$\text{arg max} \frac{1}{T_y^\alpha} \sum^{T_y}_{t=1} \log P( y^{\langle  t \rangle} | x,  y^{\langle  1 \rangle}, …,  y^{\langle  t-1 \rangle})$$</p>
<p>如果\(\alpha=1\)，就完全用长度来归一化；如果\(\alpha=0\)，就是没有做归一化。一般会选择一个中间值。<strong>\(\alpha\)也是算法的一个超参</strong>，需要不断调整来得到最好的结果，并没有理论性验证。</p>
<p>上式也称为<strong>归一化对数概率目标（normalized log probability objective）</strong>或<strong>归一化对数似然目标（normalized log likelihood objective）</strong>。</p>
<p>总结一下如何运行beam search算法。运行beam search，会得到一系列长度的翻译语句，假如限制输出语句最长为30个单词，则得到长度从1到30的语句的概率。每个长度都会保留\(B\)个概率最高的语句。然后针对这些<strong>所有可能的输出语句</strong>，用上式给他们打分，取得分最高的一个语句作为翻译结果。</p>
<ol start="2">
<li>如何选择参数\(B\)</li>
</ol>
<p>很显然，\(B\)越大，取得的结果越良好，但计算量和内存需求也更大。</p>
<p>前面的讲解汇总\(B\)取3是比较小的，在产品中，经常可以看到\(B\)取10的情况。\(B\)的大小也是取决于应用场景的，比如论文中会看到\(B\)取值1000或者3000，主要是为了压榨出全部性能。</p>
<p>但\(B\)的提升的边际效益会递减，从1提升到3或者10的收益，要比从1000提升到3000的收益大得多。</p>
<p>另外，需要明白的是beam search和计算机中的其他搜索算法，如广度优先搜索（BFS）、深度优先搜索（DFS）不一样，<strong>它们都是精确的搜索算法</strong>。而<strong>beam search是一个近似搜索算法</strong>，并不保证搜索到实际的最大值。</p>
<h2 id="1-5-Error-analysis-in-beam-search"><a href="#1-5-Error-analysis-in-beam-search" class="headerlink" title="1.5- Error analysis in beam search"></a>1.5- Error analysis in beam search</h2><p>我们知道beam search是一个模糊的启发式搜索算法，并不保证搜索到实际的最大值，因此就需要对beam search的结果进行错误分析。我们需要<strong>分析错误的原因是RNN模型还是beam search</strong>，来指导我们的模型优化。</p>
<p>下面仍用法语翻译的例子，我们将人类翻译的结果记为\(y^*\)，算法翻译的结果记为\(\hat y\)：</p>
<p><img src="https://blog-1253739411.cos.ap-shanghai.myqcloud.com/content/2018/07/Xnip2018-07-31_21-35-25.jpg" alt="Xnip2018-07-31_21-35-25"></p>
<p>虽然对RNN增加训练样本或者增加beam search的参数\(B\)对翻译结果总是没有坏处的，但我们要知道瓶颈在哪，才能有的放矢。</p>
<p>方法很简单，直接用RNN去计算人类翻译和算法翻译的概率，即比较\(P(y^*|x)\)和\(P(\hat y|x)\)做判断：</p>
<ul>
<li>如果\(P(y^*|x) &gt; P(\hat y|x)\)，说明beam search效果不好，没有搜索到概率更大的翻译语句。</li>
<li>如果\(P(y^*|x) \le P(\hat y|x)\)，说明RNN效果不好，较好的翻译语句没有得到更大的概率。</li>
</ul>
<p>基于上面的原则，我们做一个表格，对一定数量的错误案例进行分析，<strong>统计RNN和beam search出错的占比</strong>，得到模型的瓶颈在哪里，然后针对其优化。</p>
<p>做一个错误的分析表，选取一定的数量看一看。</p>
<h2 id="1-6-Bleu-Score"><a href="#1-6-Bleu-Score" class="headerlink" title="1.6- Bleu Score"></a>1.6- Bleu Score</h2><p>TODO:<br>衡量机器翻译结果和人工翻译结果的相似性（重合性）<br>如果只翻译出一个单词呢？<br>这一集的视频太水了，都懒得剪辑了。</p>
<p>单值评价</p>
<h2 id="1-7-Attention-Model-Intuition"><a href="#1-7-Attention-Model-Intuition" class="headerlink" title="1.7- Attention Model Intuition"></a>1.7- Attention Model Intuition</h2><p><strong>注意力模型（Attention Model）</strong> 是深度学习领域最有影响力的思想之一。</p>
<ol>
<li>长序列问题</li>
</ol>
<p>之前我们使用的RNN模型，都是encoder完整输入所有待翻译的序列后，decoder再输出翻译后的序列。而面对长句子时，人类翻译者并不会一次性阅读完整个句子才翻译，而是拆分为一小段一小段的，逐段翻译，因为人们很难一次性记住很长的句子。</p>
<p>我们发现，随着句子的增长，Bleu Score开始降低：</p>
<p><img src="https://blog-1253739411.cos.ap-shanghai.myqcloud.com/content/2018/08/Xnip2018-08-01_08-08-55.jpg" alt="Xnip2018-08-01_08-08-55"></p>
<ol start="2">
<li>Attention model</li>
</ol>
<p>使用注意力模型可以显著解决这个问题，attention model受人类翻译的启发，<strong>我们并不希望神经网络每次记忆很长的文字，而是每次处理一段文字，这样在处理长句的情况下Bleu Score不降低</strong>。</p>
<p>（我有个疑问，为什么一定要和人一样，人是因为有记忆限制的缺陷。如果不用注意力模型，翻译效果会不会比人更好？）</p>
<p>首先，我们使用一个BRNN作为编码器，BRNN输出每个单词丰富的特征：</p>
<p><img src="https://blog-1253739411.cos.ap-shanghai.myqcloud.com/content/2018/08/Xnip2018-08-01_08-26-52.jpg" alt="Xnip2018-08-01_08-26-52"></p>
<p>然后我们用另外一个单向RNN作为解码器，为了避免两个RNN的激活函数混淆，此处解码器RNN用\(s\)表示激活函数。</p>
<p>我们的问题是，解码器的每一个时间步应该查看被翻译语句的哪些部分，即比如\(s^\langle  1 \rangle\)应考虑编码器哪些输出。在注意力模型里，我们使用<strong>注意力权重（attention weights）</strong> 来表示对句子每个部分的权重，这个权重记为\(\alpha^{\langle t,t’\rangle}\)，其中上标含义是，解码器的第\(t\)个时间步，对编码器第\(t’\)个输出的注意力权重。我们用\(c\)表示编码器激活函数在注意力权重加权后的结果，将\(c\)输入到解码器用来生成翻译语句，如下图：</p>
<p><img src="https://blog-1253739411.cos.ap-shanghai.myqcloud.com/content/2018/08/Xnip2018-08-01_18-22-35.jpg" alt="Xnip2018-08-01_18-22-35"></p>
<p>第二个输出也是类似，激活函数\(s^\langle  2 \rangle\)的输入来自注意力权重加权的新结果\(c\)，当然同时会将上一个时间步输出的翻译结果也加入；以此类推，如下图：</p>
<p><img src="https://blog-1253739411.cos.ap-shanghai.myqcloud.com/content/2018/08/Xnip2018-08-01_19-59-46.jpg" alt="Xnip2018-08-01_19-59-46"></p>
<p>TODO: 注意力模型的思想主要来源论文：[Bahdanau et. al., 2014. Neural machine translation by jointly learning to align and translate]</p>
<h2 id="1-8-Attention-Model"><a href="#1-8-Attention-Model" class="headerlink" title="1.8- Attention Model"></a>1.8- Attention Model</h2><p>上一节介绍了注意力模型的Intuition，现在我们形式化定义。模型和上一节一样，下面是一个BRNN的编码器，上面是一个多对多的RNN解码器，如下图：</p>
<p><img src="https://blog-1253739411.cos.ap-shanghai.myqcloud.com/content/2018/08/Xnip2018-08-01_20-38-45.jpg" alt="Xnip2018-08-01_20-38-45"></p>
<p>其中每一个输出，单独拿出来如下：</p>
<p><img src="https://blog-1253739411.cos.ap-shanghai.myqcloud.com/content/2018/08/Xnip2018-08-01_20-54-58.jpg" alt="Xnip2018-08-01_20-54-58"></p>
<p>我们把编码器BRNN的每个单元的前向和后向激活函数分别记为\(\overrightarrow a^{\langle t’\rangle}\)和\(\overleftarrow a^{\langle t’\rangle}\)，为了便于说明，我们合并记为\(a^{\langle t’\rangle}\)，即：</p>
<p>$$a^{\langle t’\rangle} = (\overrightarrow a^{\langle t’\rangle}, \overleftarrow a^{\langle t’\rangle})$$</p>
<p>对解码器的第\(t\)个时间步，其输入来自编码器的注意力加权\(c\)，以及前一个时间步的激活函数\(s^{\langle  t-1 \rangle}\)，输出\(y^{\langle  t-1 \rangle}\)。其中\(c\)的计算为：</p>
<p>$$c^{\langle  t \rangle} = \sum_{t’}\alpha^{\langle t,t’\rangle}  a^{\langle t’\rangle}$$</p>
<p>显然注意力权重\(\alpha\)要满足大于0，并且和为1：</p>
<p>$$\sum_{t’}\alpha^{\langle t,t’\rangle} = 1$$</p>
<p><strong>\(\alpha^{\langle t,t’\rangle}\)，即代表了\(y^{\langle  t \rangle}\)对\(a^{\langle t’\rangle}\)的注意力大小</strong>。这个权重的计算如下：</p>
<p>$$ a^{\langle t,t’\rangle} = \frac{\text{exp}(e^{\langle t,t’\rangle})}{\sum^{T_x}_{t’=1} \text{exp}(e^{\langle t,t’\rangle})}$$</p>
<p>其中\(e\)的计算，使用一个小神经网络计算，如下：</p>
<p><img src="https://blog-1253739411.cos.ap-shanghai.myqcloud.com/content/2018/08/Xnip2018-08-01_20-52-25.jpg" alt="Xnip2018-08-01_20-52-25"></p>
<h1 id="2-Speech-recognition-Audio-data"><a href="#2-Speech-recognition-Audio-data" class="headerlink" title="2- Speech recognition - Audio data"></a>2- Speech recognition - Audio data</h1><h2 id="2-1-Speech-recognition"><a href="#2-1-Speech-recognition" class="headerlink" title="2.1- Speech recognition"></a>2.1- Speech recognition</h2><p>语音识别问题，就是将一段音频片段，自动转换为文字，seq2seq模型也可以应用于此。</p>
<p><img src="https://blog-1253739411.cos.ap-shanghai.myqcloud.com/content/2018/08/Xnip2018-08-01_21-01-06.jpg" alt="Xnip2018-08-01_21-01-06"></p>
<p>语音识别问题，在早期是通过音位（phonemes）来构建的，需要手工工程设计的基本单元（hand-engineered basic units of cells）。我们会将单词拆分为基本的音位，如the拆分为th和e的音，而quick拆分为k w i k四个音。语音学家认为用户音位表示法（phonemes representations）是做语音识别的好办法。</p>
<p>但在end-to-end的深度学习模型中，已经没有必要人工去分析音位了。通过大量的文本语音数据集，运用深度学习算法大大推进了语音识别的进程。最好的商业系统，可能会使用长达10万小时的音频训练。</p>
<p>当然，我们可以使用attention model训练语音识别：</p>
<p><img src="https://blog-1253739411.cos.ap-shanghai.myqcloud.com/content/2018/08/Xnip2018-08-01_21-14-21.jpg" alt="Xnip2018-08-01_21-14-21"></p>
<p>还有一种方法效果非常不错，它使用了CTC（Connectionist Temporal Classification）损失函数。下面简要介绍：</p>
<p>假如我们要识别的语音对应的内容是”the quick brown fox”，</p>
<p>我们将用一个输入和输出大小相同的RNN（这里使用单向RNN说明，实践中会用BRNN）：</p>
<p><img src="https://blog-1253739411.cos.ap-shanghai.myqcloud.com/content/2018/08/Xnip2018-08-01_21-36-38.jpg" alt="Xnip2018-08-01_21-36-38"></p>
<p>需要注意的是，输入的时间步通常会比输出的时间步大很多。举个例子，10秒钟的音频，如果用100Hz采样，则每秒就会产生100个样本，于是10秒的音频，会产生1000个输入，但输出的字符肯定没有这么多。CTC的做法是，允许RNN产生类似如下的输出：</p>
<blockquote>
<p>ttt_h_eeee_ _ _ \<space> _ _ _qqq_ _ _……</space></p>
</blockquote>
<p>其中_表示空白符，\<space>表示空格。将空白符分隔的字符折叠起来，然后去掉空白符，则得到了输出文本：</space></p>
<blockquote>
<p>the quick brown fox</p>
</blockquote>
<p>这样就允许整个网络有1000个有重复的字母的输出，但最终仍能得到短得多的文本输出。</p>
<p>paper参考：<a href="http://people.idsia.ch/~santiago/papers/icml2006.pdf" target="_blank" rel="noopener">Graves et al., 2006. Connectionist Temporal Classification: Labeling unsegmented sequence data with recurrent neural networks</a></p>
<h2 id="2-2-Trigger-Word-Detection"><a href="#2-2-Trigger-Word-Detection" class="headerlink" title="2.2- Trigger Word Detection"></a>2.2- Trigger Word Detection</h2><p>相比通用的语音识别系统需要大量训练数据，触发词检测（Trigger Word Detection）要简单得多，使用较小的数据量即可训练。</p>
<p>触发检测系统，主要用于一些设备的唤醒，比如苹果的Siri、智能音箱等。下面介绍一种实现算法。</p>
<p>现在有一个这样的RNN结构，我们要做的就是把一个音频片段计算出它的声谱图特征（spectrogram features），得到特征向量\(x^{\langle  1 \rangle}, x^{\langle  2 \rangle}, x^{\langle 3\rangle}, …\)，然后输入到RNN中，最后要做的就是定义目标标签\(y\)。假如某一点是出现了触发词，那么在这之前的标签都设置为0，而这一点的设置为1；如果过了一段时间，触发词再次被说了一次，再次把这个点设置为1。</p>
<p><img src="https://blog-1253739411.cos.ap-shanghai.myqcloud.com/content/2018/08/Xnip2018-08-01_21-54-41.jpg" alt="Xnip2018-08-01_21-54-41"></p>
<p><img src="https://blog-1253739411.cos.ap-shanghai.myqcloud.com/content/2018/08/Xnip2018-08-01_21-53-37.jpg" alt="Xnip2018-08-01_21-53-37"></p>
<p>上面的算法有一个明显的缺点是，构建了一个<strong>很不平衡的训练集</strong>，即0比1要多得多。但有一个简单粗暴的办法可以解决，是的训练更容易。我们在一个时间步上输出1后，在变回0之前多输出几次1，或者在固定的一段时间内输出多个1.</p>
<h1 id="3-Conclusion"><a href="#3-Conclusion" class="headerlink" title="3- Conclusion"></a>3- Conclusion</h1><h2 id="3-1-Conclusion-and-thank-you"><a href="#3-1-Conclusion-and-thank-you" class="headerlink" title="3.1- Conclusion and thank you"></a>3.1- Conclusion and thank you</h2><p>deeplearning.ai课程结束了！与开篇的AI is the new electricity的口号相呼应，Andrew Ng激励我们：</p>
<blockquote>
<p>Deep learning is a super power! If that isn’t a superpower I don’t know what is.</p>
</blockquote>
<p><img src="https://blog-1253739411.cos.ap-shanghai.myqcloud.com/content/2018/08/DL-IS-SUPERPOWER.jpg" alt=""></p>

            </div>
            <hr/>

            
            <style>
    #reward {
        margin: 40px 0;
        text-align: center;
    }

    #reward .reward-link {
        font-size: 1.88rem;
    }

    #reward .btn-floating:hover {
        box-shadow: 0 6px 12px rgba(0, 0, 0, 0.2), 0 5px 15px rgba(0, 0, 0, 0.2);
    }

    #rewardModal {
        width: 320px;
        height: 350px;
    }

    #rewardModal .reward-title {
        margin: 15px auto;
        padding-bottom: 5px;
    }

    #rewardModal .modal-content {
        padding: 10px;
    }

    #rewardModal .close {
        position: absolute;
        right: 15px;
        top: 15px;
        color: rgba(0, 0, 0, 0.5);
        font-size: 1.3rem;
        line-height: 20px;
        cursor: pointer;
    }

    #rewardModal .reward-tabs {
        margin: 0 auto;
        width: 210px;
    }

    .reward-tabs .tabs {
        height: 38px;
        margin: 10px auto;
        padding-left: 0;
    }

    .reward-tabs .tabs .tab {
        height: 38px;
        line-height: 38px;
    }

    .reward-tabs .tab a {
        color: #fff;
        background-color: #ccc;
    }

    .reward-tabs .tab a:hover {
        color: #fff;
    }

    .reward-tabs .wechat-tab .active {
        color: #fff;
        background-color: #22AB38;
    }

    .reward-tabs .alipay-tab .active {
        color: #fff;
        background-color: #019FE8;
    }

    .reward-tabs .reward-img {
        width: 210px;
        height: 210px;
    }
</style>

<div id="reward">
    <a class="reward-link btn-floating btn-large waves-effect waves-light red">赏</a>

    <!-- Modal Structure -->
    <div id="rewardModal" class="modal">
        <div class="modal-content">
            <a class="close"><i class="fa fa-close"></i></a>
            <h4 class="reward-title">走一波赞赏</h4>
            <div class="reward-content">
                <div class="reward-tabs">
                    <ul class="tabs">
                        <li class="tab wechat-tab waves-effect waves-light"><a class="active" href="#wechat">微信</a></li>
                        <li class="tab alipay-tab waves-effect waves-light"><a href="#alipay">支付宝</a></li>
                    </ul>
                    <div id="wechat">
                        <img src="/medias/reward/wechat.png" class="reward-img" alt="微信打赏二维码">
                    </div>
                    <div id="alipay">
                        <img src="/medias/reward/alipay.jpg" class="reward-img" alt="支付宝打赏二维码">
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>

<script>
    $(function () {
        $('#reward .reward-link').on('click', function () {
            $('#rewardModal').openModal();
        });

        $('#rewardModal .close').on('click', function () {
            $('#rewardModal').closeModal();
        });
    });
</script>
            

            <link rel="stylesheet" type="text/css" href="/libs/share/css/share.min.css">

<div id="article-share">
    
    <div class="social-share" data-disabled="qzone, qq, douban"></div>
    
</div>

<script src="/libs/share/js/social-share.min.js"></script>

            <div class="reprint">
                <p>
                    <span class="reprint-tip">Reprint please specify: </span>
                    <a href="https://dybl.github.io/blog" class="b-link-green">FunU</a>
                    <i class="fa fa-angle-right fa-lg fa-fw text-color"></i>
                    <a href="/2018/11/01/ml-c5-w3-seq2seq/" class="b-link-green">ml-c5-w3-seq2seq</a>
                </p>
            </div>
        </div>
    </div>

    
        <link rel="stylesheet" href="/libs/gitalk/gitalk.css">
<link rel="stylesheet" href="/css/my-gitalk.css">

<div class="card gitalk-card" data-aos="fade-up">
    <div id="gitalk-container" class="card-content"></div>
</div>

<script src="/libs/gitalk/gitalk.min.js"></script>
<script>
    let gitalk = new Gitalk({
        clientID: 'f120cd4af15e4483511b',
        clientSecret: '0cbbd24d59314d1948516419787059bd2e44cfa7',
        repo: 'https://github.com/dybl/blog_backup',
        owner: 'dybl',
        admin: null,
        id: '2018-11-01T22-26-24',
        distractionFreeMode: false  // Facebook-like distraction free mode
    });

    gitalk.render('gitalk-container');
</script>
    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">Previous</div>
            <div class="card">
                <a href="/2018/11/13/recognize-captchas/">
                    <div class="card-image">
                        
                        
                        <img src="/medias/featureimages/16.jpg" class="responsive-img" alt="recognize-captchas">
                        
                        <span class="card-title">recognize-captchas</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary">
运行环境
Python 3.x.x
需要使用的第三方库在requirements.txt，如下：


numpy
imutils
sklearn
tensorflow
keras
opencv2-python

pip3 install </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="fa fa-clock-o fa-fw icon-date"></i>2018-11-13
                        </span>
                        <span class="publish-author">
                            
                            <i class="fa fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/categories/Arithmetic/" class="post-category" target="_blank">
                                    Arithmetic
                                </a>
                            
                            <a href="/categories/Arithmetic/ML/" class="post-category" target="_blank">
                                    ML
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/tags/python/" target="_blank">
                        <span class="chip bg-color">python</span>
                    </a>
                    
                    <a href="/tags/tensorflow/" target="_blank">
                        <span class="chip bg-color">tensorflow</span>
                    </a>
                    
                    <a href="/tags/opencv/" target="_blank">
                        <span class="chip bg-color">opencv</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">Next</div>
            <div class="card">
                <a href="/2018/11/01/ubuntu-docker-wordpress/">
                    <div class="card-image">
                        
                        
                        <img src="/medias/featureimages/6.jpg" class="responsive-img" alt="ubuntu-docker-wordpress">
                        
                        <span class="card-title">ubuntu-docker-wordpress</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary">How to build wordpress in docker?
本文将介绍如何在`ubuntu 16.04 LTS`中安装`docker community edition`(`即docker社区版`)并用`docker-compose</div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="fa fa-clock-o fa-fw icon-date"></i>2018-11-01
                            </span>
                        <span class="publish-author">
                            
                            <i class="fa fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/categories/Linux/" class="post-category" target="_blank">
                                    Linux
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/tags/docker/" target="_blank">
                        <span class="chip bg-color">docker,</span>
                    </a>
                    
                    <a href="/tags/wordpress/" target="_blank">
                        <span class="chip bg-color">wordpress</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>
</div>
    

</main>

<footer class="page-footer bg-color">
    <div class="container row center-align">
        <div class="col s12 m8 l8 copy-right">
            2018 &copy;<a href="http://dybl.github.io" target="_blank"> FunYou co.,ltd</a>
        </div>
        <div class="col s12 m4 l4 social-link">
            <a href="https://github.com/" class="tooltipped" target="_blank" data-tooltip="-> Gituhb"
                data-position="top" data-delay="50">
                <i class="fa fa-github fa-lg"></i>
            </a>
            <a href="mailto:xxx@example.com" class="tooltipped" target="_blank" data-tooltip="-> Email" data-position="top"
                data-delay="50">
                <i class="fa fa-envelope fa-lg"></i>
            </a>
            <a href="https://twitter.com" class="tooltipped" data-tooltip="-> Twitter" data-position="top" data-delay="50">
                <i class="fa fa-twitter fa-lg"></i>
            </a>
            <a href="https://facebook.com" class="tooltipped" data-tooltip="-> Facebook" data-position="top" data-delay="50">
                <i class="fa fa-facebook-official fa-lg"></i>
            </a>
            <a href="https://instagram.com" class="tooltipped" data-tooltip="-> Ins" data-position="top" data-delay="50">
                <i class="fa fa-instagram fa-lg"></i>
            </a>
        </div>
    </div>
</footer>

<div class="progress-bar"></div>



<!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title">Search</span>
            <input type="search" id="searchInput" name="s" placeholder="Please enter a search keyword"
                   class="search-input" autofocus="">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script src="/js/search.js"></script>
<script type="text/javascript">
    searchFunc("/" + "search.xml", 'searchInput', 'searchResult');
</script>
<!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fa fa-angle-double-up"></i>
    </a>
</div>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
            processEscapes: true,
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
    });
    
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';                 
        }       
    });
    </script>
    

    <script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML' async></script>
    


<script src="/libs/materialize/js/materialize.min.js"></script>
<script src="/libs/masonry/masonry.pkgd.min.js"></script>
<script src="/libs/aos/aos.js"></script>
<script src="/libs/scrollprogress/scrollProgress.min.js"></script>
<script src="/libs/lightGallery/js/lightgallery-all.min.js"></script>
<script src="/js/matery.js"></script>
<script src="/live2dw/lib/L2Dwidget.min.js?0c58a1486de42ac6cc1c59c7d98ae887"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"scale":1,"hHeadPos":0.5,"vHeadPos":0.618,"jsonPath":"/live2dw/assets/assets/haruto.model.json"},"display":{"superSample":2,"width":125,"height":250,"position":"right","hOffset":0,"vOffset":-20},"mobile":{"show":false,"scale":0.5},"react":{"opacityDefault":1,"opacityOnHover":0.7},"log":false});</script></body>
</html>